from groq import Groq
from dotenv import load_dotenv
import os
from tts_gemini import text_to_speech


class ConversationAgent:
    def __init__(self):
        load_dotenv()
        self.client = Groq(api_key=os.environ["GROQ_KEY"])
        self.initiate_history()

    @staticmethod
    def read_file(file_path: str) -> str:
        with open(file_path, "r", encoding="utf-8") as file:
            return file.read()

    def initiate_history(self) -> None:
        self.history = [
            {
                "role": "system",
                "content": ConversationAgent.read_file("./context.txt"),
            }
        ]

    def update_history(self, role: str, content: str) -> None:
        self.history.append(
            {
                "role": role,
                "content": content,
            }
        )

    # ========= STT : audio (bytes) -> texte =========
    def stt_audio_to_text_bytes(
        self,
        audio_bytes: bytes,
        filename: str = "audio.m4a",
        stt_model: str = "whisper-large-v3",
        language: str = "fr",
        prompt: str | None = None,
    ) -> str:
        transcription = self.client.audio.transcriptions.create(
            file=(filename, audio_bytes),
            model=stt_model,
            response_format="json",
            language=language,
            temperature=0.0,
            prompt=prompt,
        )
        return (transcription.text or "").strip()

    # ========= LLM texte -> texte =========
    def ask_llm(self, user_interaction: str, model: str) -> str:
        self.update_history(role="user", content=user_interaction)

        response = self.client.chat.completions.create(
            messages=self.history,
            model=model,
        ).choices[0].message.content

        self.update_history(role="assistant", content=response)

        return response

    # ========= Pipeline : TEXTE -> LLM -> TTS =========
    def ask_llm_with_tts(
        self,
        user_interaction: str,
        model: str,
        voice_name: str = "Kore",
    ) -> tuple[str, bytes]:
        answer_text = self.ask_llm(
            user_interaction=user_interaction,
            model=model,
        )

        audio_bytes = text_to_speech(
            text=answer_text,
            voice_name=voice_name,
            as_wav=True,
        )

        return answer_text, audio_bytes

    # ========= Pipeline : AUDIO -> STT -> LLM -> TTS =========
    def ask_llm_from_audio_with_tts(
        self,
        audio_bytes: bytes,
        model: str,
        filename: str = "audio.m4a",
        stt_model: str = "whisper-large-v3",
        language: str = "fr",
        voice_name: str = "Kore",
        prompt: str | None = None,
    ) -> tuple[str, bytes]:
        user_text = self.stt_audio_to_text_bytes(
            audio_bytes=audio_bytes,
            filename=filename,
            stt_model=stt_model,
            language=language,
            prompt=prompt,
        )

        if not user_text:
            user_text = "Je n’ai pas compris ton audio, parle plus clairement gamin."

        answer_text = self.ask_llm(
            user_interaction=user_text,
            model=model,
        )

        audio_answer = text_to_speech(
            text=answer_text,
            voice_name=voice_name,
            as_wav=True,
        )

        return answer_text, audio_answer

    def terminal_user_interface(self, model: str) -> None:
        while True:
            user_interaction = input("Vous : ")
            if user_interaction.lower() == "exit":
                break
            elif user_interaction == "":
                print("Jarvis : Vous n'avez rien à dire ?")
            else:
                answer_text, _ = self.ask_llm_with_tts(
                    user_interaction=user_interaction,
                    model=model,
                )
                print(f"Jarvis : {answer_text}")


if __name__ == "__main__":
    conversation_agent = ConversationAgent()
    conversation_agent.terminal_user_interface(model="openai/gpt-oss-120b")
